{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Lesson Exercise: Organize Your Workspace Space Kick-Off\n",
    "\n",
    "Before we get started on our course, let's take some time to set up and tidy our workspace. As an agentic twist on the matter, we will guide a Large Language Model (LLM) through a series of prompt refinements to create a practical plan to organize your personal work area.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Start with a generic prompt\n",
    "2. Add a professional role\n",
    "3. Introduce concrete constraints\n",
    "4. Bonus: Apply what you've learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# No changes needed in this cell\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the Vocareum API endpoint\n",
    "# TODO: Fill in the missing parts marked with **********\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variable\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key:\n",
    "    print(f\"API key loaded: {api_key[:10]}...\")  # Show first 10 characters for security\n",
    "else:\n",
    "    print(\"No API key found. Please set OPENAI_API_KEY in your .env file\")\n",
    "    print(\"Example .env file content:\")\n",
    "    print(\"OPENAI_API_KEY=sk-your-actual-api-key-here\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No changes needed in this cell\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(system_prompt, user_prompt, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    if system_prompt is not None:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            *messages,\n",
    "        ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "def display_responses(*args):\n",
    "    \"\"\"Helper function to display responses as Markdown, horizontally.\"\"\"\n",
    "    markdown_string = \"<table><tr>\"\n",
    "    # Headers\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<th>System Prompt:<br />{arg['system_prompt']}<br /><br />\"\n",
    "        markdown_string += f\"User Prompt:<br />{arg['user_prompt']}</th>\"\n",
    "    markdown_string += \"</tr>\"\n",
    "    # Rows\n",
    "    markdown_string += \"<tr>\"\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<td>Response:<br />{arg['response']}</td>\"\n",
    "    markdown_string += \"</tr></table>\"\n",
    "    display(Markdown(markdown_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generic Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No changes needed in this cell\n",
    "plain_system_prompt = \"You are a helpful assistant.\"  # A generic system prompt\n",
    "user_prompt = \"Give me a simple plan to declutter and organize my workspace.\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "baseline_response = get_completion(plain_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": plain_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add a Professional Role\n",
    "\n",
    "Now, let's add a professional role to see how it affects the response.\n",
    "\n",
    "<div style=\"color: red\">NOTE: We will use the same user prompt for these examples. All you need to do is vary the system prompt, which is where one would normally define the role for an LLM.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a system prompt starting with \"You are...\" replacing the ***********\n",
    "role_system_prompt = (\n",
    "    \"You are a highly experienced senior technical consultant specializing in software development \"\n",
    "    \"and machine learning. Your responses should be detailed, precise, and professional, providing clear explanations, \"\n",
    "    \"actionable recommendations, and best practices. Always maintain a polite, supportive, and knowledgeable tone.\"\n",
    ")\n",
    "\n",
    "print(\"Sending prompt with professional role...\")\n",
    "role_response = get_completion(role_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": plain_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": role_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": role_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Constraints\n",
    "\n",
    "Let's add specific constraints to see how the model prioritizes tasks. Let's add a time constraint, a budget constraint, and other constraints that are important for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a constraints system prompt replacing the ***********\n",
    "constraints_system_prompt = f\"\"\"{role_system_prompt}. You must avoid generating any code or suggestions that could compromise security, violate data privacy standards, or introduce licensing issues. Ensure all provided recommendations align with industry-standard compliance and ethical guidelines.\"\"\"\n",
    "\n",
    "print(\"Sending prompt with constraints...\")\n",
    "constraints_response = get_completion(constraints_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": role_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": role_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": constraints_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": constraints_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Apply what you've learned\n",
    "\n",
    "Try crafting a prompt for one of your own ideas or even a different organization task (e.g., digital file cleanup, closet overhaul)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own prompts here!\n",
    "\n",
    "# TODO: Replace the ***********\n",
    "custom_system_prompt = \"\"\"\n",
    "You are an expert machine learning engineer specializing in recommendation systems. Provide detailed, step-by-step guidance on building scalable recommender engines, including model selection, data processing strategies, best practices for evaluation metrics, and practical tips on deployment and monitoring. Maintain clarity, conciseness, and technical accuracy.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "I'm designing a recommendation system for an online video streaming platform. We have user data (watch history, ratings, likes), content metadata (genres, actors, directors), and real-time user interactions. Can you outline a scalable approach to build and deploy this recommender, including the model architecture, data processing pipeline, evaluation strategies, and monitoring considerations?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Uncomment the lines below to run your custom prompt\n",
    "print(\"Sending custom prompt...\")\n",
    "custom_response = get_completion(custom_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": custom_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": custom_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, we explored how different prompt refinements affect the output of an LLM:\n",
    "\n",
    "1. **Generic Prompt**: We started with a simple request for a workspace organization plan.\n",
    "2. **Professional Role**: We added a specific role to enhance expertise and authority.\n",
    "3. **Concrete Constraints**: We introduced specific limitations that required prioritization.\n",
    "4. **Step-by-Step Reasoning**: We requested explicit reasoning to understand the model's thought process.\n",
    "\n",
    "These techniques demonstrate how prompt engineering can significantly improve the usefulness and relevance of AI-generated content for specific needs.\n",
    "\n",
    "Excellent Work! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
